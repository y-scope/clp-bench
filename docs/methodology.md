# Methodology
## Overview
We use a Linux server with Intel Xeon E5-2630v3 processor and 128GB of DDR4 memory. Both the uncompressed and compressed logs are stored in 7200RPM SATA HDD. We use `clp-bench` to run the benchmark, which is a python package. We use different datasets based on types as illustrated as the following.

### Type
There are two types `Unstructured` and `Semi-Structured` of logs:
+ **Unstructured logs** are log entries that don’t follow a predefined format or structure. These logs can contain free-form text, making it harder to automatically parse or analyze them. Instead of having a clear pattern like JSON, unstructured logs are often just raw text with timestamps, messages, and additional details mixed in. An example:
```
2018-06-05 06:15:29,701 DEBUG org.apache.hadoop.util.Shell: setsid exited with exit code 0
```

The dataset of the `Unstructured` type is generated by running workloads from the HiBench Benchmark Suite on three Hadoop clusters. Each cluster contains one node. The dataset is sizable, amounting to approximately 258GB of data. The dataset reflects various activities and performance characteristics of the Hadoop during HiBench execution, making it suitable for evaluating log compression and search tools.

+ **Semi-structured logs** don’t have a rigid format, they do contain identifiable elements or patterns that make them easier to parse than unstructured logs. Semi-structured logs usually have a few consistent fields (e.g., timestamps, log levels) but allow for variability in other fields and their contents of the messages.

```
{"t":{"$date":"2023-03-21T23:34:54.576-04:00"},"s":"I",  "c":"NETWORK",  "id":4915701, "ctx":"-","msg":"Initialized wire specification","attr":{"spec":{"incomingExternalClient":{"minWireVersion":0,"maxWireVersion":17},"incomingInternalClient":{"minWireVersion":0,"maxWireVersion":17},"outgoing":{"minWireVersion":6,"maxWireVersion":17},"isInternalClient":true}}}
```

The dataset of the `Semi-structured` type is generated by MongoDB, which is a single file consists of 64.8GB of log data with 186,287,600 records. It is generated using open-source benchmarking tools, specifically by running benchmarks like HiBench and YCSB. These benchmarks simulate various database operations that trigger logging activity in MongoDB. Each operation, such as insertions, updates, and queries, produces log entries that are captured in the dataset.

### Metric
There are two metrics `Hot Run` and `Cold Run` of the benchmarking:
+ **Hot Run** is that we run the query benchmark immediately after ingesting data. (*TODO: In the future, we will run the benchmark until the results remain the same for the warm-up, then take the results.*)
+ **Cold Run** refers to running the query benchmark on the tool after shutting it down post-data ingestion and restarting it with a cold boot. (*TODO: In the future, we plan to implement a more comprehensive approach by clearing the page cache and swap space to ensure a completely cold cache environment.*)

### Ingest Time
We collect timestamps before and after ingesting data and use this end-to-end latency as the ingest time.  

### Compressed Size
We use `du {archives} -bc` to measure the compressed data size after all data is ingested. The smaller the compressed size is, the better the compression ratio.

### Average Memory Usage
We monitor the average memory usage for both `ingest` and `query` stages. The `ingest` stage is we ingest the data and the `query` stage is we run the query benchmark. There are two main methods to get the memory usage based on tools:
+ For tools running with multiple microservices (e.g., Loki), we use `docker stats` to poll the total memory usage of all related containers with fixed time interval (e.g., 10s), then calculate the average as the average memory usage.
+ For other tools, we use `ps` and collect the data of `RSS` field for all related processes. The total value is the memory usage, and we poll it regularly to calculate the average memory usage.

### Query Latency  
We collect timestamps before sending the query and after receiving the query results and use this end-to-end latency as the query latency.

## Benchmarking Target Specifics
### CLP and CLP-S
To begin, download the latest released binary or clone the most recent code from our [Github Repo](https://github.com/y-scope/clp) and compile it locally by following the instructions provided [here](https://docs.yscope.com/clp/main/dev-guide/index.html).

We use the `glt` binary for CLP (unstructured logs) and `clp-s` for CLP-S (semi-structured logs). The benchmark is run using Docker, so you can build a Docker image as described [here](https://docs.yscope.com/clp/main/dev-guide/components-core/index.html).

#### CLP
For CLP, start by creating a `yaml` configuration file like the example below:
```
system_metric:
  enable: True
  memory:
    ingest_polling_interval: 5
    run_query_benchmark_polling_interval: 5
    
glt:
  container_id: xiaochong-clp-oss-jammy-xiaochong
  binary_path: /home/xiaochong/develop/clp-bench-prototype/tests/clp-unstructured/glt-local-compiled
  data_path: /home/xiaochong/develop/clp-bench-prototype/tests/clp-unstructured/offline-mode-data
  dataset_path: /home/xiaochong/develop/clp-bench-prototype/tests/datasets/hadoop-small
  queries:
    - '" org.apache.hadoop.hdfs.server.common.Storage: Analyzing storage directories for bpid "'
    - '" org.apache.hadoop.hdfs.server.datanode.DataNode: DataTransfer, at "'
    - '" INFO org.apache.hadoop.yarn.server.nodemanager.containermanager.container.ContainerImpl: Container "'
    - '" DEBUG org.apache.hadoop.mapred.ShuffleHandler: verifying request. enc_str="'
    - '" to pid 21177 as user "'
    - '" 10000 reply: "'
    - '" 10 reply: "'
    - '" 178.2 MB "'
    - '" 1.9 GB "'
    - '"job_1528179349176_24837"'
    - '"blk_1075089282_1348458"'
    - '"hdfs://master:8200/HiBench/Bayes/temp/worddict"'
    - '" abcde "'
```

Note that in this configuration (which also applies for the following benchmark targets):
+ `data_path` specifies where the tool will store ingested data.
+ `dataset_path` refers to the location of data that is ready to be ingested.

With the `yaml` file configured and the container running, you can execute the following command to run the benchmarks:
```
clp-bench -t GLT -m {mode} -c {path-to-yaml}
```
Here, `mode` can be set to `hot`, `cold`, or `query-only` (which also applies for the following benchmark targets). For more details, run `clp-bench --help`.

No preprocessing is needed for the raw data. During data ingestion, `clp-bench` will execute:
```
docker exec {container_id} {binary_path} c {data_path} {dataset_path}
```

For query benchmarking, `clp-bench` runs the following command for each query:
```
docker exec {container_id} {binary_path} s {data_path} {query}
```
To verify the results, `clp-bench` appends `| wc -l` to count the matched log lines, ensuring the tool correctly identifies matches during the query process (which also applies for the following benchmark targets).

For memory monitoring, `clp-bench` periodically executes `ps aux` (based on the intervals specified under `system.memory` in the `yaml` file) within the container, checking the `RSS` field for processes associated with `binary_path` and `data_path`:
```
docker exec {container_id} ps aux
```
The averages of the memory usage collected during ingestion and query benchmark stages will be calculated respectively. 

#### CLP-S
CLP-S follows a similar setup to CLP, with slight differences. Below is an example of a `yaml` configuration file for CLP-S:
```
system_metric:
  enable: True
  memory:
    ingest_polling_interval: 5
    run_query_benchmark_polling_interval: 5
    
clp_s:
  container_id: xiaochong-clp-oss-jammy-xiaochong
  binary_path: /home/xiaochong/develop/clp-bench-prototype/tests/clp-json/clp-json-x86_64-v0.1.3/bin/clp-s-local-compiled
  data_path: /home/xiaochong/develop/clp-bench-prototype/tests/clp-json/offline-mode-data
  dataset_path: /home/xiaochong/develop/clp-bench-prototype/tests/datasets/mongodb-single
  queries:
    - 'attr.tickets:*'
    - 'id: 22419'
    - 'attr.message.msg: log_release* AND attr.message.session_name: connection'
    - 'ctx: initandlisten AND (NOT msg: "WiredTigermessage" OR attr.message.msg: log_remove*)'
    - 'c: WTWRTLOG AND attr.message.ts_sec > 1679490000'
    - 'ctx: FlowControlRefresher AND attr.numTrimmed: 0'
```
The difference is the query's format. The query for JSON logs uses [KQL](https://docs.yscope.com/clp/main/user-guide/reference-json-search-syntax.html).

Similarly, with the `yaml` file configured and the container running, you can execute the following command to run the benchmarks:
```
clp-bench -t CLPS -m {mode} -c {path-to-yaml}
```

Like CLP, no preprocessing of raw data is required. The commands for data ingestion and query benchmarking remain the same. Memory monitoring is also performed using `ps aux` and checking the `RSS` field.

### grep
*grep* is a command-line tool in Unix/Linux systems used to search for specific patterns within files or text. We use it as a baseline of unstructured log query benchmark.

Below is an example of `yaml` file of it:
```
system_metric:
  enable: False
    
grep:
  dataset_path: /home/xiaochong/develop/clp-bench-prototype/tests/datasets/hadoop-small
  queries:
    - '" org.apache.hadoop.hdfs.server.common.Storage: Analyzing storage directories for bpid "'
    - '" org.apache.hadoop.hdfs.server.datanode.DataNode: DataTransfer, at "'
    - '" INFO org.apache.hadoop.yarn.server.nodemanager.containermanager.container.ContainerImpl: Container "'
    - '" DEBUG org.apache.hadoop.mapred.ShuffleHandler: verifying request. enc_str="'
    - '" to pid 21177 as user "'
    - '" 10000 reply: "'
    - '" 10 reply: "'
    - '" 178.2 MB "'
    - '" 1.9 GB "'
    - '"job_1528179349176_24837"'
    - '"blk_1075089282_1348458"'
    - '"hdfs://master:8200/HiBench/Bayes/temp/worddict"'
    - '" abcde "'
```
Since *grep* does not ingest data, so there is no difference between modes, you can randomly choose one:
```
clp-bench -t Grep -m {mode} -c {path-to-yaml}
```

For the query benchmark, `clp-bench` executes the following command for each query:
```
grep -r {query} {dataset_path}
```

### Loki
[Loki](https://grafana.com/oss/loki/) runs with two microservices. Loki itself is working as a backend which ingests the data sent by the log collector. We use [Promtail](https://grafana.com/docs/loki/latest/send-data/promtail/) as its log collector. These two are running in two containers communicated through REST APIs. For query benchmark, we use [LogCLI](https://grafana.com/docs/loki/latest/query/logcli/) to execute queries.

We haven't integrated launching and ingesting for Loki into `clp-bench` yet, so you may need to manually launch and ingest data first, then use `clp-bench` to run the query benchmark.

#### Launch
To run Loki, create a `loki-config.yaml` configuration file as the following (see details [here](https://grafana.com/docs/loki/latest/configure/)):
```
auth_enabled: false

server:
  http_listen_port: 3100
  grpc_listen_port: 9096

common:
  instance_addr: 127.0.0.1
  path_prefix: /tmp/loki
  storage:
    filesystem:
      chunks_directory: /tmp/loki/chunks
      rules_directory: /tmp/loki/rules
  replication_factor: 1
  ring:
    kvstore:
      store: inmemory

query_range:
  results_cache:
    cache:
      embedded_cache:
        enabled: true
        max_size_mb: 1000

schema_config:
  configs:
    - from: 1972-10-24
      store: tsdb
      object_store: filesystem
      schema: v13
      index:
        prefix: index_
        period: 24h

limits_config:
  reject_old_samples: false
  ingestion_rate_mb: 1000

ruler:
  alertmanager_url: http://localhost:9093
```

Then launch a container for Loki with the following command (assuming you are currently at the directory which contains the above mentioned `yaml` configuration file):
```
docker run --name loki -d -v $(pwd):/mnt/config -p 3100:3100 grafana/loki:3.0.0 -config.file=/mnt/config/loki-config.yaml
```

To run Promtail, create a `promtail-config.yaml` file as the following:
```
server:
  http_listen_port: 9080
  grpc_listen_port: 0

positions:
  filename: /tmp/positions.yaml

clients:
  - url: http://loki:3100/loki/api/v1/push

scrape_configs:
- job_name: system
  static_configs:
  - targets:
      - localhost
    labels:
      job: benchlogs
      __path__: /mnt/datasets/hadoop/worker*/*
```

Note that `__path__` should be the pattern of directories which contain the log files.

Then launch a container for Promtail with the following command (assuming you are currently at the directory which contains the above mentioned `yaml` configuration file):
```
docker run --name promtail -d -v $(pwd):/mnt/config -v /path/to/hadoop-log-datasets:/mnt/datasets/hadoop --link loki grafana/promtail:3.0.0 -config.file=/mnt/config/promtail-config.yaml
```

When containers for Loki and Promtail have been launched, run the following command until it prints `ready`, then Loki will start ingesting data:
```
curl -G http://localhost:3100/ready
```

#### Ingest
Loki ingests data automatically when connects to Promtail. We do not do any preprocessing for the dataset.

We use `docker stats` to get the memory usage of ingesting data periodically (the frequency should be consistent with the `yaml` configuration file for `clp-bench as mentioned in the next section) until the ingestion finishes:
```
docker stats loki promtail --no-stream
```

Since Loki does not have prompt when ingestion finishes, we monitor the current ingested data by the following command:
```
curl -G http://localhost:3100/metrics | grep 'loki_distributor_bytes_received_total'
```

When the above command prints the size that equals to the size of the dataset, it means the ingestion finishes. Then we run the following command to get the time spent for ingesting data:
```
curl -G http://localhost:3100/metrics | grep 'loki_request_duration_seconds_sum{method="POST",route="loki_api_v1_push"'
```

#### Query Benchmarking
An example of `yaml` configuration file of Loki for `clp-bench` to run the query benchmark is like:
```
system_metric:
  enable: True
  memory:
    ingest_polling_interval: 5
    run_query_benchmark_polling_interval: 5
    
loki:
  logcli_binary_path: /home/xiaochong/develop/loki/logcli
  job: benchlogs
  limit: 2000000
  batch: 1000
  from: '2024-10-08T10:00:00Z'
  to: '2024-10-09T10:00:00Z'
  interval: 30
  queries:
    - '" org.apache.hadoop.hdfs.server.common.Storage: Analyzing storage directories for bpid "'
    - '" org.apache.hadoop.hdfs.server.datanode.DataNode: DataTransfer, at "'
    - '" INFO org.apache.hadoop.yarn.server.nodemanager.containermanager.container.ContainerImpl: Container "'
    - '" DEBUG org.apache.hadoop.mapred.ShuffleHandler: verifying request. enc_str="'
    - '" to pid 21177 as user "'
    - '" 10000 reply: "'
    - '" 10 reply: "'
    - '" 178.2 MB "'
    - '" 1.9 GB "'
    - '"job_1528179349176_24837"'
    - '"blk_1075089282_1348458"'
    - '"hdfs://master:8200/HiBench/Bayes/temp/worddict"'
    - '" abcde "'
```

Note that in this configuration: 
+ `loki.job` should match the `job` of the `labels` in the `promtail-config.yaml` (see the example of `promtail-config.yaml`).
+ `limit` should be at least the maximum number of matched log lines of the query.
+ `batch` is maximum number of matched log lines that Loki will send to the client at once.
+ `from` and `to` define the rough wall-clock time range of ingesting data. In this example, it means that data ingestion happened between `2024-10-08T10:00:00Z` and `2024-10-09T10:00:00Z`.
+ `interval` defines the time range, in minutes, that Loki will use to query log lines ingested within that period. For example, setting an interval of 30 means the time range between from and to will be divided into 30-minute slices. Loki will run the query on log lines ingested during each of these slices. For each query, `clp-bench` instructs Loki to execute the query across all time slices, ensuring the entire dataset is covered.

With the `yaml` configuration file for `clp-bench` (which is different from the `yaml` file for Loki and Promtail containers) and running containers of Loki and Promtail (all data must have been ingested), we can run the following command to run the query benchmark:
```
clp-bench -t GrafanaLoki -m query-only -c {path-to-loki-clp-bench-yaml}
```

For each query, `clp-bench` run the following command. Note that the `time_slice_start` and `time_slice_to` are the start and end timestamps for each time slice, `clp-bench` will iterate all time slices between `from` and `to` in the configuration to cover the entire dataset.
```
{logcli_binary_path} query '{ job="{job}" } |~ {query}' --limit={limit} --batch={batch} --from="{time_slice_start}" --to="{time_slice_to}"
```

For measuring memory usage during query execution, we employ the same method used for data ingestion.

### Elasticsearch
We benchmarked Elasticsearch for both unstructured and semi-structured logs. They are basically the same, except the query's format and the preprocessing of the semi-structured log dataset. Generally, we use single-node deployment of Elasticsearch, disabling the security feature of [xpack](https://www.elastic.co/guide/en/elasticsearch/reference/current/security-settings.html).

For unstructured logs, an example of the `yaml` configuration file is like:
```
system_metric:
  enable: True
  memory:
    ingest_polling_interval: 10
    run_query_benchmark_polling_interval: 1

elasticsearch:
  container_id: elasticsearch-xiaochong
  launch_script_path: /home/assets/start-ela.sh
  compress_script_path: /home/assets/compress.py
  search_script_path: /home/assets/query.py
  terminate_script_path: /home/assets/stop-ela.sh
  memory_polling_script_path: /home/assets/poll_mem.py
  data_path: /var/lib/elasticsearch
  log_path: /var/log/elasticsearch
  dataset_path: /home/datasets/worker*/worker*/*log*
  queries:
    - '{"query": {"bool": {"must": {"match_phrase": {"log_line": " org.apache.hadoop.hdfs.server.common.Storage: Analyzing storage directories for bpid "}}}}, "size": 10000}'
    - '{"query": {"bool": {"must": {"match_phrase": {"log_line": " org.apache.hadoop.hdfs.server.datanode.DataNode: DataTransfer, at "}}}}, "size": 10000}'
    - '{"query": {"bool": {"must": {"match_phrase": {"log_line": " INFO org.apache.hadoop.yarn.server.nodemanager.containermanager.container.ContainerImpl: Container "}}}}, "size": 10000}'
    - '{"query": {"bool": {"must": {"match_phrase": {"log_line": " DEBUG org.apache.hadoop.mapred.ShuffleHandler: verifying request. enc_str="}}}}, "size": 10000}'
    - '{"query": {"bool": {"must": {"match_phrase": {"log_line": " to pid 21177 as user "}}}}, "size": 10000}'
    - '{"query": {"bool": {"must": {"match_phrase": {"log_line": " 10000 reply: "}}}}, "size": 10000}'
    - '{"query": {"bool": {"must": {"match_phrase": {"log_line": " 10 reply: "}}}}, "size": 10000}'
    - '{"query": {"bool": {"must": {"match_phrase": {"log_line": " 178.2 MB "}}}}, "size": 10000}'
    - '{"query": {"bool": {"must": {"match_phrase": {"log_line": " 1.9 GB "}}}}, "size": 10000}'
    - '{"query": {"bool": {"must": {"match_phrase": {"log_line": "job_1528179349176_24837"}}}}, "size": 10000}'
    - '{"query": {"bool": {"must": {"match_phrase": {"log_line": "blk_1075089282_1348458"}}}}, "size": 10000}'
    - '{"query": {"bool": {"must": {"match_phrase": {"log_line": "hdfs://master:8200/HiBench/Bayes/temp/worddict"}}}}, "size": 10000}'
    - '{"query": {"bool": {"must": {"match_phrase": {"log_line": " abcde "}}}}, "size": 10000}'
```

For semi-structured logs, an example of the `yaml` configuration file is like:
```
system_metric:
  enable: True
  memory:
    ingest_polling_interval: 10
    run_query_benchmark_polling_interval: 1

elasticsearch:
  container_id: elasticsearch-semi-xiaochong
  launch_script_path: /home/assets/start-ela.sh
  compress_script_path: /home/assets/compress.py
  search_script_path: /home/assets/query.py
  terminate_script_path: /home/assets/stop-ela.sh
  memory_polling_script_path: /home/assets/poll_mem.py
  data_path: /var/lib/elasticsearch
  log_path: /var/log/elasticsearch
  dataset_path: /home/datasets/mongod.log
  queries:
    - '{"query": {"exists": {"field": "attr.tickets"}}, "size": 10000}'
    - '{"query": {"term": {"id": 22419}}, "size": 10000}'
    - '{"query": {"bool": {"must": [{"wildcard": {"attr.message.msg": "log_release*"}}, {"match": {"attr.message.session_name": "connection"}}]}}, "size": 10000}'
    - '{"query": {"bool": {"must": [{"match": {"ctx": "initandlisten"}}], "should": [{"wildcard": {"attr.message.msg": "log_remove*"}}, {"bool": {"must_not": [{"match_phrase": {"msg": "WiredTiger message"}}]}}], "minimum_should_match": 1}}, "size": 10000}'
    - '{"query": {"bool": {"must": [{"match": {"c": "WTWRTLOG"}}, {"range": {"attr.message.ts_sec": {"gt": 1679490000}}}]}}, "size": 10000}'
    - '{"query": {"bool": {"must": [{"match": {"ctx": "FlowControlRefresher"}}, {"match": {"attr.numTrimmed": 0}}]}}, "size": 10000}'
```

Note that there are some scripts used by `clp-bench` can be found under `clp-bench-prototype/assets/elasticsearch-unstructured` (for unstructured logs) and `clp-bench-prototype/assets/elasticsearch` (for semi-structured logs). There are also `Dockerfile` can be found under these directories, which are used to build the containers with `docker_build.sh`. Once the containers are built, use `docker_run.sh` to launch them. The script requires two arguments: the first is the absolute path of the dataset on the host, and the second is the absolute path where the ingested data will be stored on the host.

With the `yaml` file and the running container corresponding to the log dataset type, we can run the following command to benchmark Elasticsearch:
```
# For unstructured log dataset
clp-bench -t ElasticsearchUnstructured -m {mode} -c {path-to-yaml}
# For semi-structured log dataset
clp-bench -t Elasticsearch -m {mode} -c {path-to-yaml}
```

For data ingestion, no preprocessing is needed for unstructured log datasets. However, JSON logs generated by MongoDB require some adjustments to be searchable by Elasticsearch. For details, refer to the `traverse_data` function in `clp-bench-prototype/assets/elasticsearch/compress.py`. The general approach involves reorganizing certain fields by moving them to outer or inner objects to ensure the queries function correctly. For both types of dataset, `clp-bench` uses `streaming_bulk` from `elasticsearch.helpers` to ingest data (refer to either `clp-bench-prototype/assets/elasticsearch-unstructured/compress.py` or `clp-bench-prototype/assets/elasticsearch/compress.py`).

For query benchmarking, `clp-bench` also uses functionality from `elasticsearch` python package to execute queries. For details, refer to `execute_query_without_cache` functions in `clp-bench-prototype/assets/elasticsearch-unstructured/query.py` and `clp-bench-prototype/assets/elasticsearch/query.py`.

For memory monitoring, similar to CLP and CLP-S, `clp-bench` uses `ps aux` and checks the `RSS` field.